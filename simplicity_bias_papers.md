<h1 id="papers-on-simplicity-bias">Papers on Simplicity Bias</h1>
<hr>
<h2 id="papers">Papers</h2>
<hr>
<table>
<thead>
<tr>
<th>Papers</th>
<th>Link</th>
<th>Code</th>
<th>Published in</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan=4><p align='center'><b><em>2015</em></b></p></td>
</tr>
<tr>
<td>In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning</td>
<td><a href="https://arxiv.org/abs/1412.6614">Paper</a></td>
<td></td>
<td>ICLR 2015</td>
</tr>
<tr>
<td colspan=4><p align='center'><b><em>2017</em></b></p></td>
</tr>
<tr>
<td>SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data</td>
<td><a href="https://arxiv.org/abs/1710.10174">ArXiV</a></td>
<td></td>
<td>Pre-print (2017)</td>
</tr>
<tr>
<td>Critical Learning Periods in Deep Neural Networks</td>
<td><a href="https://arxiv.org/abs/1711.08856">ArXiV</a></td>
<td><a href="https://github.com/uw-mad-dash/Accordion">Code</a></td>
<td>Pre-print (2017)</td>
</tr>
<tr>
<td>A closer look at memorization in deep networks</td>
<td><a href="https://proceedings.mlr.press/v70/arpit17a/arpit17a.pdf">Paper</a></td>
<td></td>
<td>ICML 2017</td>
</tr>
<tr>
<td colspan=4><p align='center'><b><em>2018</em></b></p></td>
</tr>
<tr>
<td>Deep learning generalizes because the parameter-function map is biased towards simple functions</td>
<td><a href="https://arxiv.org/abs/1805.08522">ArXiV</a></td>
<td>-</td>
<td>Pre-print (2018)</td>
</tr>
<tr>
<td colspan=4><p align='center'><b><em>2019</em></b></p></td>
</tr>
<tr>
<td>SGD on Neural Networks Learns Functions of Increasing Complexity</td>
<td><a href="https://dl.acm.org/doi/10.5555/3454287.3454601">Paper</a></td>
<td></td>
<td>NeurIPS 2019</td>
</tr>
<tr>
<td>Intrinsic dimension of data representations in deep neural networks</td>
<td><a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/cfcce0621b49c983991ead4c3d4d3b6b-Paper.pdf">Paper</a></td>
<td></td>
<td>NeurIPS 2019</td>
</tr>
<tr>
<td>Random deep neural networks are biased towards simple functions</td>
<td><a href="https://proceedings.neurips.cc/paper/2019/file/feab05aa91085b7a8012516bc3533958-Paper.pdf">Paper</a></td>
<td>-</td>
<td>NeurIPS 2019</td>
</tr>
<tr>
<td colspan=4><p align='center'><b><em>2020</em></b></p></td>
</tr>
<tr>
<td>The Pitfalls of Simplicity Bias in Neural Networks</td>
<td><a href="https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf">Paper</a></td>
<td><a href="https://github.com/harshays/simplicitybiaspitfalls">Code</a></td>
<td>NeurIPS 2020</td>
</tr>
<tr>
<td>Shortcut learning in deep neural networks</td>
<td><a href="https://www.nature.com/articles/s42256-020-00257-z">Paper</a> <a href="https://arxiv.org/abs/2004.07780">ArXiV</a></td>
<td></td>
<td>Nature Machine Intelligence 2020</td>
</tr>
<tr>
<td>Gradient Starvation: A Learning Proclivity in Neural Networks</td>
<td><a href="https://arxiv.org/pdf/2011.09468.pdf">Paper</a></td>
<td><a href="https://github.com/mpezeshki/Gradient_Starvation">Code</a></td>
<td>NeurIPS 2020</td>
</tr>
<tr>
<td>High-Frequency Component Helps Explain the Generalization of Convolutional Neural Networks</td>
<td><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_High-Frequency_Component_Helps_Explain_the_Generalization_of_Convolutional_Neural_Networks_CVPR_2020_paper.pdf">Paper</a></td>
<td><a href="https://github.com/HaohanWang/HFC">Code</a></td>
<td>CVPR 2020</td>
</tr>
<tr>
<td>Hierarchical nucleation in deep neural networks</td>
<td><a href="https://proceedings.neurips.cc/paper/2020/file/54f3bc04830d762a3b56a789b6ff62df-Paper.pdf">Paper</a></td>
<td><a href="https://github.com/diegodoimo/hierarchical_nucleation">Code</a></td>
<td>NeurIPS 2020</td>
</tr>
<tr>
<td>Do deep neural networks learn shallow learnable examples first?</td>
<td><a href="https://openreview.net/forum?id=HkxHv4rn24">Paper</a></td>
<td><a href="https://github.com/karttikeya/Shallow_to_Deep/">Code</a></td>
<td>ICML 2020</td>
</tr>
<tr>
<td>What shapes feature representations? Exploring datasets, architectures, and training</td>
<td><a href="https://proceedings.nips.cc/paper/2020/file/71e9c6620d381d60196ebe694840aaaa-Paper.pdf">Paper</a></td>
<td></td>
<td>NeurIPS 2020</td>
</tr>
<tr>
<td colspan=4><p align='center'><b><em>2021</em></b></p></td>
</tr>
<tr>
<td>Just Train Twice: Improving Group Robustness without Training Group Information</td>
<td><a href="http://proceedings.mlr.press/v139/liu21f/liu21f.pdf">Paper</a></td>
<td><a href="https://github.com/anniesch/jtt">Code</a></td>
<td>ICML 2021</td>
</tr>
<tr>
<td>Adversarial Perturbations Are Not So Weird: Entanglement of Robust and Non-Robust Features in Neural Network Classifiers</td>
<td><a href="https://arxiv.org/pdf/2102.05110.pdf">ArXiV</a></td>
<td></td>
<td>Pre-print (2021)</td>
</tr>
<tr>
<td>The Low-Rank Simplicity Bias in Deep Networks</td>
<td><a href="https://minyoungg.github.io/overparam/resources/overparam-v3.pdf">Paper</a></td>
<td><a href="https://github.com/minyoungg/overparam">Code</a></td>
<td>TMLR 2021</td>
</tr>
<tr>
<td>Do Input Gradients Highlight Discriminative Features?</td>
<td><a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/0fe6a94848e5c68a54010b61b3e94b0e-Paper.pdf">Paper</a></td>
<td><a href="https://github.com/harshays/inputgradients">Code</a></td>
<td>NeurIPS 2021</td>
</tr>
<tr>
<td>Can contrastive learning avoid shortcut solutions?</td>
<td><a href="https://proceedings.neurips.cc/paper/2021/file/27934a1f19d678a1377c257b9a780e80-Paper.pdf">Paper</a></td>
<td><a href="https://github.com/joshr17/IFM">Code</a></td>
<td>NeurIPS 2021</td>
</tr>
<tr>
<td colspan=4><p align='center'><b><em>2022</em></b></p></td>
</tr>
<tr>
<td>Evading the Simplicity Bias: Training a Diverse Set of Models Discovers Solutions with Superior OOD Generalization</td>
<td><a href="https://ehsanabb.github.io/assets/files/Evading_the_Simplicity_Bias_CVPR_2022_paper.pdf">Paper</a></td>
<td></td>
<td>CVPR 2022</td>
</tr>
<tr>
<td>Self-supervised debiasing using low rank regularization</td>
<td><a href="https://openreview.net/forum?id=PHpK5B2iGpq">OpenReview</a></td>
<td>-</td>
<td>Pre-print (2022)</td>
</tr>
<tr>
<td>Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations</td>
<td><a href="https://arxiv.org/abs/2204.02937">ArXiV</a></td>
<td>-</td>
<td>Pre-print (2022)</td>
</tr>
<tr>
<td colspan=4><p align='center'><b><em>2023</em></b></p></td>
</tr>
<tr>
<td>Simplicity Bias in 1-Hidden Layer Neural Networks</td>
<td><a href="https://neurips.cc/virtual/2023/poster/71765">Paper</a></td>
<td>-</td>
<td>NeurIPS 2023 (Poster)</td>
</tr>
<tr>
<td>Overcoming Simplicity Bias in Deep Networks using a Feature Sieve</td>
<td><a href="https://proceedings.mlr.press/v202/tiwari23a/tiwari23a.pdf">Paper</a></td>
<td>-</td>
<td>ICML 2023</td>
</tr>
<tr>
<td>Delving Deep into Simplicity Bias for Long-Tailed Image Recognition</td>
<td><a href="https://arxiv.org/abs/2302.03264">ArXiv</a></td>
<td>-</td>
<td>Pre-print (2023)</td>
</tr>
<tr>
<td>Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions</td>
<td><a href="https://aclanthology.org/2023.acl-long.317.pdf">Paper</a></td>
<td>-</td>
<td>ACL 2023</td>
</tr>
<tr>
<td>Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression</td>
<td><a href="https://proceedings.mlr.press/v202/xue23d/xue23d.pdf">Paper</a></td>
<td></td>
<td>ICML 2023</td>
</tr>
<tr>
<td>Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious Correlations?</td>
<td><a href="https://arxiv.org/abs/2308.00473">ArXiV</a></td>
<td>-</td>
<td>Pre-print (2023)</td>
</tr>
<tr>
<td>Neural networks trained with SGD learn distributions of increasing complexity</td>
<td><a href="https://proceedings.mlr.press/v202/refinetti23a/refinetti23a.pdf">Paper</a></td>
<td><a href="https://github.com/sgoldt/dist_inc_comp.">Code</a></td>
<td>NeurIPS 2023</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="blogs">Blogs</h2>
<hr>
<table>
<thead>
<tr>
<th>Blog</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>A brief note on Simplicity Bias</td>
<td><a href="https://www.lesswrong.com/posts/Gyggp2DJRMRLSnhid/a-brief-note-on-simplicity-bias-1">Link</a></td>
</tr>
<tr>
<td>Neural networks are fundamentally (almost) Bayesian</td>
<td><a href="https://towardsdatascience.com/neural-networks-are-fundamentally-bayesian-bee9a172fad8">Link</a></td>
</tr>
<tr>
<td>Deep Neural Networks are biased, at initialisation, towards simple functions</td>
<td><a href="https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">Link</a></td>
</tr>
</tbody>
</table>
<hr>
