# Papers on Simplicity Bias
---
## Papers
---
| Papers | Link | Code | Published in |
|---|---|---|---|
| Deep learning generalizes because the parameter-function map is biased towards simple functions | [ArXiV](https://arxiv.org/abs/1805.08522) | - | Pre-print |
| Random deep neural networks are biased towards simple functions | [Paper](https://proceedings.neurips.cc/paper/2019/file/feab05aa91085b7a8012516bc3533958-Paper.pdf) | - | NeurIPS 2019 |
| The Pitfalls of Simplicity Bias in Neural Networks | [Paper](https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf) | [Code](https://github.com/harshays/simplicitybiaspitfalls) | NeurIPS 2020 |
| Simplicity Bias in 1-Hidden Layer Neural Networks | [Paper](https://neurips.cc/virtual/2023/poster/71765) | - | NeurIPS 2023 (Poster) |
| Overcoming Simplicity Bias in Deep Networks using a Feature Sieve | [Paper](https://proceedings.mlr.press/v202/tiwari23a/tiwari23a.pdf) | - | ICML 2023 |
| The Low-Rank Simplicity Bias in Deep Networks | [Paper](https://minyoungg.github.io/overparam/resources/overparam-v3.pdf) | [Code](https://github.com/minyoungg/overparam) | TMLR 2021 |
| Do Input Gradients Highlight Discriminative Features? | [Paper](https://proceedings.neurips.cc/paper_files/paper/2021/file/0fe6a94848e5c68a54010b61b3e94b0e-Paper.pdf) | [Code](https://github.com/harshays/inputgradients)| NeurIPS 2021 |
| Self-supervised debiasing using low rank regularization | [OpenReview](https://openreview.net/forum?id=PHpK5B2iGpq) | - | Pre-print |
| Delving Deep into Simplicity Bias for Long-Tailed Image Recognition | [ArXiv](https://arxiv.org/abs/2302.03264) | - | Pre-print |
| Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions | [Paper](https://aclanthology.org/2023.acl-long.317.pdf) | - | ACL 2023 |
| Evading the Simplicity Bias: Training a Diverse Set of Models Discovers Solutions with Superior OOD Generalization | [Paper](https://ehsanabb.github.io/assets/files/Evading_the_Simplicity_Bias_CVPR_2022_paper.pdf) | | CVPR 20222 |
| Can contrastive learning avoid shortcut solutions? | [Paper](https://proceedings.neurips.cc/paper/2021/file/27934a1f19d678a1377c257b9a780e80-Paper.pdf) | [Code](https://github.com/joshr17/IFM) | NeurIPS 2021 |
| Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression | [Paper](https://proceedings.mlr.press/v202/xue23d/xue23d.pdf)| | ICML 2023 |
| Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations | [ArXiV](https://arxiv.org/abs/2204.02937) | - | - |
| Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious Correlations? | [ArXiV](https://arxiv.org/abs/2308.00473) | - | - |
| Shortcut learning in deep neural networks | [Paper](https://www.nature.com/articles/s42256-020-00257-z) [ArXiV](https://arxiv.org/abs/2004.07780) | | Nature Machine Intelligence 2020 |
| Gradient Starvation: A Learning Proclivity in Neural Networks | [Paper](https://arxiv.org/pdf/2011.09468.pdf) | [Code](https://github.com/mpezeshki/Gradient_Starvation) | NeurIPS 2020 |
---
## Blogs
---
| Blog | Link |
|---|---|
| A brief note on Simplicity Bias | [Link](https://www.lesswrong.com/posts/Gyggp2DJRMRLSnhid/a-brief-note-on-simplicity-bias-1) |
| Neural networks are fundamentally (almost) Bayesian | [Link](https://towardsdatascience.com/neural-networks-are-fundamentally-bayesian-bee9a172fad8) |
| Deep Neural Networks are biased, at initialisation, towards simple functions | [Link](https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99) |
